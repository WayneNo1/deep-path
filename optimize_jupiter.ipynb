{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import gym\n",
    "from dlpf.agents import DqnAgent, RandomAgent, FlatAgent, FlatAgentWithLossLogging\n",
    "from dlpf.io import *\n",
    "from data_shuffle import *\n",
    "\n",
    "logger = init_log(out_file = 'import.log', stderr = False)\n",
    "to_import = True\n",
    "if to_import:\n",
    "    import_tasks_from_xml_to_compact('data/sample/raw/', 'data/sample/imported/')\n",
    "    shuffle_imported_paths(to_split=True, val=False)\n",
    "    shuffle_imported_maps(to_split=True, val=False)\n",
    "\n",
    "logger = init_log(out_file = 'testbed.log', stderr = False)\n",
    "\n",
    "\n",
    "env = gym.make('PathFindingByPixel-v3')\n",
    "env.configure(tasks_dir = os.path.abspath('data/sample/imported/'), monitor_scale = 10, map_shape = (501, 501))\n",
    "env.monitor.start('data/sample/results/basic_dqn', force=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    reslog = []\n",
    "    agent = FlatAgentWithLossLogging(state_size = env.observation_space.shape,\n",
    "                     number_of_actions = env.action_space.n,\n",
    "                     save_name = env.__class__.__name__)\n",
    "    agent.build_model(number_of_neurons=space['neurons'],\n",
    "                      desc_name=space['desc'],\n",
    "                      loss_fn=space['lf'],\n",
    "                      dropout1=space['dropout1'],\n",
    "                      activation=space['activation'])\n",
    "\n",
    "    episode_count = space['episodes']\n",
    "    max_steps = 500\n",
    "\n",
    "    for _ in xrange(episode_count):\n",
    "        env.mode = 'train'\n",
    "        observation = env.reset()\n",
    "        agent.new_episode()\n",
    "        walls = 0\n",
    "        for __ in range(max_steps):\n",
    "            action, values = agent.act(observation, epsilon=0.05+0.95*0.999**(_))\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if info:\n",
    "                walls += 1\n",
    "            agent.observe(reward, action)\n",
    "            if done:\n",
    "                break\n",
    "        steps = __\n",
    "        #if _ % 100 == 99:\n",
    "        #    print 'iteration:', _ + 1\n",
    "        #    agent.plot_layers(to_save='iteration'+str(_+1))\n",
    "        if _ % 10 == 9:\n",
    "            agent.train_with_full_experience()\n",
    "\n",
    "    for _ in xrange(episode_count):\n",
    "        env.mode = 'test'\n",
    "        observation = env.reset()\n",
    "        agent.new_episode()\n",
    "        walls = 0\n",
    "        for __ in range(max_steps):\n",
    "            action, values = agent.act(observation, epsilon=0)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if info:\n",
    "                walls += 1\n",
    "            agent.observe(reward, action)\n",
    "            if done:\n",
    "                break\n",
    "        steps = __\n",
    "    if env.heights[env.cur_task.start[0]][env.cur_task.start[1]] > 0:\n",
    "        reslog.append(steps+walls/env.heights[env.cur_task.start[0]][env.cur_task.start[1]])\n",
    "    else:\n",
    "        reslog.append(3)\n",
    "        print 'something weird happened'\n",
    "    print 'result: ', sum(reslog)/len(reslog)\n",
    "    return {'loss': sum(reslog)/len(reslog), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}